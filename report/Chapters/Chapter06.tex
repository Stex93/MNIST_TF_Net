%*****************************************
\chapter{Conclusions and future work}\label{ch:conclusions}
%*****************************************

Research in machine learning attempts to make better representations, more performing, possibly limiting computational costs in terms of time and space. Some remarkable results were obtained with a new kind of \acs{DNN}: Convolutional Neural Networks (\acsp{CNN}). Thus we analyzed a \acs{CNN} model built with the TensorFlow framework, an open source distribution by Google.

TensorFlow was chosen because of its flexible architecture, which allows to deploy the computation to one or more \acsp{CPU} or \acsp{GPU} in a desktop, server, or mobile device with a single API.

\begin{description}
	\item[Chapter \ref{ch:conv_nets}] introduced the main features and building blocks of convolutional neural netowrks architecture, with special regard to the parameters which have been taken into account when designing one of them.
	
	\item[Chapter \ref{ch:tensorflow_basics}] focused on the TensorFlow framework; the main data structures and available operations were presented and a very simple model was shown.
	
	\item[Chapter \ref{ch:basic_tf_model}] described the Convolutional Neural Network model built for classifying task of images from the \acs{MNIST} dataset, focusing on every layer and the respective lines of code, and finally presenting attempts to change the architecture and the parameters settings.
	
	\item[Chapter \ref{ch:data_analysis}] showed the main data related results obtained during the experiments, with respect to different perspectives of analysis on the data gathered with diverse tested solutions.
\end{description}

Finally we can draw the conclusions of the work done in this project: \acsp{CNN} are a more powerful kind of Deep Neural Network, because they allow to obtain better results on more complex tasks, with not exorbitant training computational costs. The reducing of computational costs and memory space is obtained with fewer weights, i.e. fewer connections between neurons, which are also more performing than fully connected neural networks.

With the Convolutional Neural Network model built and the achieved results, we can conclude that the performances are very satisfactory, with limited computational costs and execution times.

However our work is not on a siding, there could be a lot questions still to answer continuing this project. Here are some of them.

About the padding:
\begin{itemize}
	\item Could the padding type be changed?
	\item What could it happen with different dimensions and colors of the padding?
	\item How much the padding affects the results and the performances of a \acsp{CNN}?
	\item Can we do without the padding?
	\item Could there be other solutions to avoid the usage of padding?
	\item In more complex images with non uniform background color, is 0 padding a good solution? 
\end{itemize}

About the tuning of hyperparameters, in particular the number of filters, which reflects in the number of features the network identify:
\begin{itemize}
	\item Could be formalized a procedure to precisely determine the number of filters to learn?
	\item Which could be a good trade off between training times and final accuracy according to necessities?
\end{itemize}

Last but not least, transfer learning:
\begin{itemize}
	\item Could it be possible to use the same trained network retraining the fully connected layer for new tasks?
	\item An example could be our convolutional ceural cetwork which recognizes only the digits. Could we train another network on letters and then transfer its learning to our model?
\end{itemize}

Our analysis turned out to be very interesting, but possible future projects could be even more interesting and useful in machine learning and Deep Neural Network research.