%************************************************
 \chapter{Introduction}\label{ch:introduction}
%************************************************

Machine learning is becoming more and more important in a huge number of domains. Softwares able to increase their performances by means of knowledge about the world are now used in smartphones as well as social networks, search engines and intelligent cars, too. At the same time, tasks are becoming increasingly hard and powerful models have to be developed.

Deep learning is the branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data.
Research in this area attempts to make better representations and create models to learn these representations from large-scale unlabeled data. Some of the representations are inspired by advances in neuroscience and are loosely based on interpretation of information processing and communication patterns in a nervous system, such as neural coding which attempts to define a relationship between various stimuli and associated neuronal responses in the brain.

In this context neural networks, and in particular deep neural networks, play a fundamental role. Various deep learning architectures such as convolutional deep neural networks, deep belief networks and recurrent neural networks have been applied to fields like computer vision, automatic speech recognition, natural language processing, audio recognition and bioinformatics where they have been shown to produce state-of-the-art results on various tasks. Actually, in many of these fields the amount of data to be processed is too large to use traditional Multi Layer Feed Forward Neural Networks with success.

At the same time, in the last few years, a lot of different frameworks for deep learning have been released. For instance, in 2014 Berkeley Vision and Learning Center released Caffe [\cite{jia2014caffe}]. Theano [\cite{2016arXiv160502688full}] is a numerical computation library for Python developed by a machine learning group at the Université de Montréal. Microsoft, too, developed CNTK,  its own deep learning framework.

Among all these tools Google's TensorFlow deserves a special mention.  It is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows to deploy the computation to one or more \acsp{CPU} or \acsp{GPU} in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team, within Google's Machine Intelligence research organization, for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. Actually, nowadays dozens of commercial products by Google use TensorFlow: speech recognition, Gmail, Google Photos, Google Search, and so on.

The aim of our project was to learn some basic concepts about convolutional neural network as well as familiarize with the TensorFlow framework. In particular, we tried to deploy a convolutional model built with TensorFlow following the official tutorial. Since we were interested in the field of image recognition, our model was focused on the MNIST dataset of handwritten digits. The network is designed for recognizing handwritten digits and, if properly tuned, reaches pretty satisfying levels of accuracy. A simple \ac{GUI} was developed, too, in order to test the trained network. We then tried to tune the network in order to reach a good trade-off between accuracy and training time, acting upon both the architecture and the network hyperparameters.

This report is organized as follows:

\begin{description}
	
	\item[Chapter \ref{ch:conv_nets}] introduces the main features and building blocks of convolutional neural networks, with special regard to the parameters that have to be taken into account when designing one of them;
	
	\item[Chapter \ref{ch:tensorflow_basics}] is focused on the TensorFlow frameworks; the main data-structures and available operations are presented and a very simple model is shown;
	
	\item[Chapter \ref{ch:basic_tf_model}] describes the model built for the MNIST dataset and the experiments made on that architecture;
	
	\item[Chapter \ref{ch:data_analysis}] shows the main results obtained during the experiments, with respect to the accuracies reached with the different tested solutions;
	
	\item[Chapter \ref{ch:conclusions}] finally draws some conclusions about the experiments, and about convolutional neural networks in general, and proposes a possible direction for future work.
	
\end{description}