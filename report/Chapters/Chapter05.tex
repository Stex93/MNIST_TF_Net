%*****************************************
\chapter{Analysis of collected data}\label{ch:data_analysis}
%*****************************************

In order to reach satisfying conclusions, we will analyze carefully the collected data every time the structure of the network has been changed, trying to understand how it performs, what kind of adjustment we can do to improve the results, and comparing the different structured networks performances on the same classifying task with MNIST images.

\section{Gathering data}

To begin with, we will describe the gathering data process focusing on what, when and why we recorded data during network executions. It is important to understand the core of our analysis and the conclusions we will reach at the end of this project report.

\subsection{What, when, why}

Now will follow an accurate description of the data we gathered, focusing also on when they were recorded and explaining reasons which moved to. Choosing what and when to record data is very important, because that data project us into the TensorFlow model and permit to understand behaviour, performance and results.

Gathered data can be summarized in accuracy during training phase and test accuracy after training, both strongly connected to the number of features we set to the convolutional network. Now we explain them in detail:

\paragraph{Training accuracy}

An important measurement is the accuracy during training phase because it allows us to focus on learning trend and discover the behaviour of networks with different number of features. As we will see later with the help of some graphics, we can deduce lots of useful information. These values are between 0 and 1 and were recorded every 10 epochs, guaranteeing dense and rich sampling for a good analysis.

\paragraph{Final test accuracy}

An important value as the previous is the measurement of accuracy in testing phase, after training. With these values we can analyze performances of networks with different number of features. Focusing on these results we will conclude possible guidelines useful to set a number of features commensurate to our necessities.

\paragraph{Execution times}

Measurements of execution times were not made with precision or recorded, but we took notes about faster or slower nets to understand how structural changes or different number of features can affect training duration.

These informations, in relation to analysis on performances in training and testing phases, will help us to draw conclusions about diverse structures and parameterized \acs{CNN}.

\subsection{Storage format}

Data gathered during executions were first saved in arrays and then written to a CSV file. For each different network, a CSV file was created with recorded data of training accuracy during all executions. So, in each column of the file we find the training accuracy of an execution sampled every 10 epochs.

Regarding measurements of accuracy after training, a unique CSV file was created with all test accuracy for each different network and for each execution. Data are organized by rows, on a row we find: the number of features of the network, the values of test accuracy, a final average value of all test accuracy values for the network. These average values will be the measure to evaluate and compare networks with different numbers of features.

\section{Executions and parameters choice}

This short section will introduce and explain some contextual informations about how and why we set number of executions and parameters values in diverse convolutional networks. These choices are preserved on all executions and are fundamental in our incoming analysis.

\subsection{Number of executions}

Gathering data of a single execution and with too large sampling did not give us satisfying and considerable graphics because data were not sufficient and lines too variable. To overcome these problems we executed multiple times training and test of the same TensorFlow model, a good compromise between availability of time, computing power and sufficient amount of data is $30$ executions, but previous attempts were limited to $10$ executions.

A higher number of executions produce a bigger amount of data, more complete and statistically more significant. Another advantage is that the lines in graphics are more stable and plain, afterwards we will see the graphics and appreciate the results.

\subsection{Number of epochs}

The first version of this TensorFlow model provided $20000$ epochs, after first executions we concluded that such a big number was exaggerate, so we reduced training epochs to $15000$. That amount is amply sufficient to observe a good convergence and stabilization of training trend, and allows us after lots of executions to save time.

Another advantage of fewer epochs is a representation of data in the graphics that is less compressed, than it turns out to be more clear, informative and readable.

\subsection{Number of features}

We executed four different TensorFlow models, the only difference is the number of features we choose for each one. Probably it is the most important and influent parameter for performance during training and test phases, and the following analysis will be based on this parameter.

We choose an increasing number of features and in the convolutional layer we followed the rule of doubling, selected values are summarized in Table \ref{tab:features_number}.

\begin{table}
	\caption{Selected numbers of features in diverse TensorFlow models}
	\label{tab:features_number}
	\centering
	\begin{tabular}{ccc}
		\hline 
		& \multicolumn{2}{c}{\textbf{Number of features}} \\ 
		& 1st conv. layer & 2nd conv. layer \\ 
		\hline 
		\textbf{Model 2\_4} & 2 & 4 \\ 
		\textbf{Model 4\_8} & 4 & 8 \\ 
		\textbf{Model 8\_16} & 8 & 16 \\ 
		\textbf{Model 16\_32} & 16 & 32 \\ 
		\hline 
	\end{tabular}
\end{table}

In next section we will also quibble on a too high number of features, trying to understand if it can be evil or not, if the network find features which are equal each other and how can realize that we have chosen satisfying values.

\section{Analysis of training accuracy}

After a long wait, necessary for introduce useful informations, finally we arrive hopefully to the most interesting section of this project report. Now we are prepared to introduce an accurate analysis of the diverse TensorFlow models described.

We will focus mainly on:
\begin{itemize}
	\item Training speed
	\item Training stability
	\item Training performance
	\item Training duration
	\item Influences of parameters on the network
\end{itemize}

For each of these perspectives we will conduct a thorough analysis with the help of graphics.

\subsection{Graphics and notations}

Always considering all graphics:
\begin{itemize}
	\item[y-axis] On this axis we find the training accuracies in the range from $0.85$ to $1$. We cut out the part of the graphic below the value $0.85$ because it is not really useful and mainly because it allows us to focus on the most interesting area.
	\item[x-axis] On this axis we find the time values: the epochs. We will consider different time windows according to the needs, but usually we find all the epochs of training phase.
	\item[Legend] In every graphics of this section will refer to the four TensorFlow model with a notation based on the number of features in the two convolutional layer: number of features in first layer + \_ + number of features in second layer.
\end{itemize}

\subsection{Training speed}

The first perspective we explore is the speed of training phase, or rather how quickly the convolutional network learns from the example images. It is interesting to inspect this, because as we shall see in the graphics, in front of a greater number of features we find a more training speed. In other words it means: more features, more rapid learning.

\begin{figure}
	\caption{Comparison of training speeds of the four TensorFlow models with different number of features}
	\label{fig:training_speed}
	\centering
	\includegraphics[width=1\textwidth]{Images/training_speed}
\end{figure}

Figure \ref{fig:training_speed} shows the first part of the training phase, about the first half. It allows us to focus on the most interesting part of the graphic for the analysis we are going to do.

Focusing on Figure \ref{fig:training_speed} we can clearly observe that more features imply a more rapid learning. We can notice this in two ways, two points of view.

Considering the same training value, e.g. $0.85$, we observe the following facts. that the model with fewer features (Model $2_4$) reaches the referring value in more than $500$ epochs.

\paragraph{Model 2\_4}

The TensorFlow model with fewer features reaches the referring value of training accuracy in more than $500$ epochs.

\paragraph{Model 4\_8}

Increasing the number of features we obtain the same training accuracy value of $0.85$ in about $350$ epochs.

\paragraph{Model 8\_16}

The model with $8$ and $16$ features on first and second convolutional layer reach that training accuracy value in about $200/250$ epochs.

\paragraph{Model 16\_32}

The last model with the higher number of features is the fastest, reaching the referring training accuracy value in about $150$ epochs.
\\
\\
Even without focusing on precise number of epochs, we can notice at a glance the slower and faster model. The same observations we just made, can be done considering a precise epoch during the first part of the training phase where the learning curve mainly grows.

\subsection{Training stability}

A second perspective in our analysis is the stability of training, particularly towards the end part of training phase. Figure \ref{fig:training_stability} shows the training curve in its adolescence to its adulthood, about from epoch $5000$ to $15000$. In this range we can observe a progressive stabilizations of learning, and similarly to training speed, here the number of features we set influence the stability.

\begin{figure}
	\caption{Comparison of training stability of the four TensorFlow models with different number of features}
	\label{fig:training_stability}
	\centering
	\includegraphics[width=1\textwidth]{Images/training_stability}
\end{figure}

Without the need for explain each curve, we can focus on ups and downs: with fewer features we find more pronunced peaks, while increasing the number of features we obtain more stable learning curve.

An important conclusion of these observations is that we can have better and more stable learning with an appropriate and sufficient number of features. In case of too low value training will be instable and require more epochs. But be careful: remember that good and stable training accuracy does not imply directly a good test accuracy, so keep your head even if you do not obtain the most stable learning curve, it could be worse (e.g. your neural network can do a really bad generalization).

\subsection{Training performance}

After an interesting analysis of how quick and stable can be the training phase, finally we can focus on the most important aspect: performances. It is clear, now, that is strictly connected to the previous perspectives, and we can naturally imagine the following fact: more feature means better results. After all we have already said, it is a logical deduction, and effectively it is true.

Figure \ref{fig:cfr_all} shows a complete graphics of the diverse learning curve of training phases in our four different TensorFlow models. Analyzing this chart we can see immediately that the features number greatly influences performances, the figure speaks for itself.

Together with the two previous perspective, we can also observe that improvements are not proportional to the number of features. Rather we have greater improvement between the models 2\_4 and 4\_8, instead of between 8\_16 and 16\_32. In the first case we can observe visible differences in training speed, stability and performance, which are less marked in the second case. We can observe much better these aspects in Figure \ref{fig:cfr_low} where the focus is on models 2\_4 and 4\_8 and Figure \ref{fig:cfr_high} where there are only models 8\_16 and 16\_32.

The observation we just made is really important because it allows us to understand with experiments the extent to which it is convenient, or we consider convenient, increase the number of features that the network should recognize. Until we consider useful a certain improvement of these perspectives, we can set higher values. 

\begin{landscape}
	\begin{figure}
		\centering
		\caption{Comparison of training accuracy curves of the four TensorFlow models with different number of features}
		\label{fig:cfr_all}
		\includegraphics[width=1\textheight]{Images/cfr_all}
	\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
	\centering
	\caption{Comparison of training accuracy curves of two TensorFlow models with lower number of features}
	\label{fig:cfr_low}
	\includegraphics[width=1\textheight]{Images/cfr_2_4_vs_4_8}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
	\centering
	\caption{Comparison of training accuracy curves of two TensorFlow models with higher number of features}
	\label{fig:cfr_high}
	\includegraphics[width=1\textheight]{Images/cfr_8_16_vs_16_32}
\end{figure}
\end{landscape}

\subsection{Training duration}

A non-trivial aspect that common people with limited computing power always consider is: how much time does it take? It is a legitimate question, and we will promptly broach the subject.

Despite we do not gathered data about execution times, as we said before, we take some notes about. Consider data in Table \ref{tab:approx_times} non precise, but indicative.

\begin{table}
	\caption{Approximate times measurements of diverse TensorFlow models executions}
	\label{tab:approx_times}
	\centering
	\begin{tabular}{cc}
		\hline 
		& \textbf{Number of features} \\ 
		\hline 
		\textbf{Model 2\_4} & 7 minutes \\ 
		\textbf{Model 4\_8} & 7 minutes \\ 
		\textbf{Model 8\_16} & 7 minutes \\ 
		\textbf{Model 16\_32} & 7 minutes \\ 
		\hline 
	\end{tabular}
\end{table}

In order to complete time measurements in Table \ref{tab:approx_times}, here are some technical informations about relevant hardware characteristics:
\begin{itemize}
	\item Ultrabook Toshiba Satellite;
	\item Processor Intel Core i5;
	\item CPU frequency $1.7$ GHz;
	\item OS Ubuntu 16.10
	\item No cluster
	\item No GPU
\end{itemize}

\subsection{Influences of parameters on the network}

What we are about to say has already been said in the previous pages, while we were analyzing all the perspectives. Now we try to summarize the aspect of how diverse numbers of features influenced network performances and times.

It is clear that better trainings can be obtained with higher number of features. But it is also clear that there in not direct proportion between number of features and training speed, stability, accuracy and time execution; so we have to conclude that increasing this key parameter is the way to improve our \acs{CNN}, but how to increase the values is at your own risk, according to:
\begin{itemize}
	\item computing power availability;
	\item time availability;
	\item desired performances.
\end{itemize}

We can not leave out an important observation: training a high-valued parameters model can be expensive, very expensive, but we have to consider that this phase is only required the first time, and then we can have a well trained network with very good performances. It might be worth it.

\section{Analysis of test accuracy}

After a long and in-depth analysis of the training phase, we can enjoy the results obtained evaluating the model in test phase. It is even more important than training accuracies because here we can analyze how powerful is the model we built and if it generalizes well.

We can evaluate generalization capabilities thanks to a part of MNIST dataset which provide images not used during training phase. Providing images the model has never seen grant us a good checking of its capabilities.

As introduced in Chapter \ref{ch:basic_tf_model} focusing on MNIST dataset, a large number of different classifiers have been trained on this dataset with different results. In Table \ref{tab:test_accuracies} we gather the test accuracies of our four TensorFlow models, here we observe clearly that we have a rapid decrease of the increasing test accuracy values. Indeed, on balance, from a model with 2 and 4 features to a model with 4 and 8 features we increase the final performance of $0.588 \%$, passing to 8 and 16 features we add just $0.257 \%$ more and finally $0.132 \%$ more. The average ratio between these values is about 2, it means that every time we double the number of features, we halve the improvement.

\begin{table}
	\caption{Test accuracies of the four TensorFlow models with different number of features}
	\label{tab:test_accuracies}
	\centering
	\begin{tabular}{cc}
		\hline
		& \textbf{Test accuracies} \\
		\hline
		\textbf{Model 2\_4} & $98.145334 \%$ \\
		\textbf{Model 4\_8} & $98.733334 \%$ \\
		\textbf{Model 8\_16} & $98.990000 \%$ \\
		\textbf{Model 16\_32} & $99.122001 \%$ \\
		\hline
	\end{tabular}
\end{table}