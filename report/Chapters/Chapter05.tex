%*****************************************
\chapter{Analysis of the collected data}\label{ch:data_analysis}
%*****************************************

In order to reach satisfying conclusions, we will analyze carefully the data collected every time the structure of the network was changed, trying to understand how it performed, what kind of adjustment we can do to improve the results, and comparing the differently structured networks performances on the same classification task with the \acs{MNIST} images.

\section{Gathering data}

To begin with, we will describe the data gathering process focusing on what, when and why we collected data during network executions. This point is important to understand the scope of our analysis and the conclusions we will reach at the end of this report.

\subsection{What, when, why}

Now will follow an description of the data we gathered, focusing also on when they were recorded and explaining why. Choosing what and when to record data is very important, because that data project us into the TensorFlow model and permit to understand behavior, performance and results.

The gathered data can be summarized in accuracy measured at different stages of the training process and test accuracy after the training, both strongly connected to the number of features we set to the convolutional network. Now we explain them in detail:

\begin{description}
	\item[Training accuracy] An important measurement is the accuracy during training phase because it allows us to focus on learning trend and compare the performances of networks with different number of features. As we will see later with the help of some charts, we can deduce lots of useful information. These values are between 0 and 1 and were recorded every 10 epochs, guaranteeing a dense and rich sampling for a good analysis.
	
	\item[Final test accuracy] An important value as the previous is the measurement of accuracy in testing phase, after training. With these values we can analyze performances of networks with different number of features. Focusing on these results we will conclude possible guidelines useful to set a number of features commensurate to our necessities.
	
	\item[Execution times] Measurements of execution times were not made with precision or recorded, but we took notes about faster and slower nets with respect to the training time to understand how structural changes or different number of features can affect the training duration.
\end{description}

These informations, in relation to analysis on performances in training and testing phases, will help us to draw some conclusions about \acsp{CNN} differently structured and parameterized.

\subsection{Storage format}

Data gathered during executions were first saved in arrays and then written to a CSV file. For each different network, a CSV file was created with recorded data about the training accuracy during every execution. So, in each column of the file we have the training accuracy values of an execution sampled every 10 epochs.

As for the accuracy measurements after the training, a single CSV file was created with all the test accuracy for each different network and for each execution. Data are organized by rows, on a row we find: the number of features of the network, the values of the test accuracy, a final average value of all the test accuracy values for the network. These average values will be the measure to evaluate and compare networks with different numbers of features detected.

\section{Executions and parameters choice}

This short section will introduce and explain some contextual informations about how and why we set certain number of executions and parameters values for the different convolutional networks. These choices were preserved on all executions.

\subsection{Number of executions}

Gathering data on a single execution and with a too large sampling did not give us satisfying and useful plots because data were not sufficient and lines too variable. To overcome these problems we executed multiple times training and test of the same TensorFlow model and averaged the results, a good compromise between availability of time, computing power and sufficient amount of data is $30$ executions, but previous attempts were limited to $10$ executions.

A higher number of executions produce a bigger amount of data, more complete and statistically more significant. Another advantage is that the lines in charts are more stable and plain, afterwards we will see the plots and appreciate the results.

\subsection{Number of epochs}

The first version of this TensorFlow model provided a training phase composed by $20000$ epochs, after some executions we concluded that such a big number was exaggerate, so we reduced the number of iterations to $15000$. That amount is widely enough to observe a good convergence and stabilization of the training trend, and allowed after lots of executions to save time.

Another advantage of fewer epochs is a representation of data in the charts that is less compressed, than it turns out to be more clear, informative and readable.

\subsection{Number of filters}

We executed four different TensorFlow models, the only difference is the number of features we chose to detect for each one. Probably it is the most important and influent parameter which affected performances during training and test phases, and the following analysis will be mainly based on this parameter.

We chose an increasing number of features and in the convolutional layer we followed the rule of doubling; the selected values are summarized in table \ref{tab:features_number}.

\begin{table}
	\centering
	\begin{tabular}{ccc}
		\hline 
		& \multicolumn{2}{c}{\textbf{Number of features}} \\ 
		& 1st conv. layer & 2nd conv. layer \\ 
		\hline 
		\textbf{Model 2\_4} & 2 & 4 \\ 
		\textbf{Model 4\_8} & 4 & 8 \\ 
		\textbf{Model 8\_16} & 8 & 16 \\ 
		\textbf{Model 16\_32} & 16 & 32 \\ 
		\hline 
	\end{tabular}
	\caption{Numbers of features selected in different TensorFlow models}
	\label{tab:features_number}
\end{table}

In next section we will also quibble on a too high number of features, trying to understand if it can be bad or not, if the network find redundant features and how can realize that we have chosen satisfying values.

\section{Analysis of the training accuracy}

After a long wait, necessary for introduce useful informations, finally we arrive hopefully to the most interesting section of this project report. Now we are prepared to introduce an accurate analysis of the different TensorFlow models described before.

We will focus mainly on:
\begin{itemize}
	\item Training speed
	\item Training stability
	\item Training performance
	\item Training time
	\item Influences of parameters on the network
\end{itemize}

For each of these perspectives we will conduct a thorough analysis with the help of plots.

\subsection{Graphics and notations}

Every chart is structured as follows:
\begin{itemize}
	\item[y-axis] On this axis we find the training accuracies in the range from $0.85$ to $1$. We cut out the part of the chart below the value $0.85$ because it is not really useful and mainly because it allows us to focus on the most interesting area;
	
	\item[x-axis] On this axis we find the number of iterations: the epochs. We will consider different time windows according to the needs, but usually we find all the epochs of training phase;
	
	\item[Legend] In every plots of this section will refer to the four TensorFlow models with a notation based on the number of features in the two convolutional layers: number of features in first layer + \_ + number of features in second layer.
\end{itemize}

\subsection{Training speed}

The first perspective we explore is the speed of training phase, in other words how quickly the convolutional network learns from the example images. It is interesting to inspect this, because as we shall see in the plots, in front of a greater number of features we find a steeper learning curve, but a slower training.

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{Images/training_speed}
	\caption{Comparison of training speeds of the four TensorFlow models with different number of features}
	\label{fig:training_speed}
\end{figure}

Figure \ref{fig:training_speed} shows the first part of the training phase, about the first half. We can clearly observe that more features imply a more rapid learning. We can notice this by two points of view.

Considering the same training accuracy value, e.g. $0.85$, we observe the following facts. The model with fewer features (Model 2\_4) reaches the referring value in more epochs than models with more features.

\paragraph{Model 2\_4}

The TensorFlow model with fewer features reaches the referring value of training accuracy in more than $500$ epochs.

\paragraph{Model 4\_8}

Increasing the number of features we obtain the same training accuracy value of $0.85$ in about $350$ epochs.

\paragraph{Model 8\_16}

The model with $8$ and $16$ features in the first and second convolutional layer reach that training accuracy value in about $200/250$ epochs.

\paragraph{Model 16\_32}

The last model with the higher number of features is the fastest, reaching the referring training accuracy value in about $150$ epochs.
\\
\\
Even without focusing on the precise number of epochs, we can notice at a glance the slower and faster model. The same observations we just made, can be done considering a precise epoch during the first part of the training phase where the learning curve mainly grows.

\subsection{Training stability}

A second perspective in our analysis is the stability of the training process, particularly towards the end part of training phase. Figure \ref{fig:training_stability} shows the training curve from its adolescence to its adulthood, about from epoch $5000$ to $15000$. In this range we can observe a progressive stabilizations of learning, and similarly to training speed, here the number of features we set influences the stability.

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{Images/training_stability}
	\caption{Comparison of training stability of the four TensorFlow models with different number of features}
	\label{fig:training_stability}
\end{figure}

Without the need to explain each curve, we can focus on oscillations: with fewer features we find more pronunced peaks, while increasing the number of features we obtain more stable learning curves.

An important conclusion of these observations is that we can have better and more stable learning with an appropriate and sufficient number of features. In case of too low values the training process will be instable and will require more epochs. But be careful: remember that good and stable training accuracy does not imply directly a good test accuracy, so keep your head even if you do not obtain the most stable learning curve.

\subsection{Training performance}

After an interesting analysis of how quick and stable can be the training phase, finally we can focus on the most important aspect: performances. It is clear, now, that this is strictly connected to the previous perspectives, and we can naturally imagine the following fact: more features mean better results. After all we have already said, it is a logical deduction, and effectively it is true.

Figure \ref{fig:cfr_all} shows a complete plots of the different learning curves of training phases in our four different TensorFlow models. Analyzing this chart we can see immediately that the features number greatly influences performances, the figure speaks for itself.

Together with the two previous perspective, we can also observe that improvements are not proportional to the number of features. Rather we have greater improvement between the models 2\_4 and 4\_8, instead of between 8\_16 and 16\_32. In the first case we can observe visible differences in training speed, stability and performance, which are less marked in the second case. We can observe much better these aspects in Figure \ref{fig:cfr_low} where the focus is on models 2\_4 and 4\_8 and Figure \ref{fig:cfr_high} where there are only models 8\_16 and 16\_32.

The observation we just made is really important because it allows us to understand with experiments the extent to which it is convenient, or we consider convenient, increase the number of features that the network should recognize. Until we consider useful a certain improvement of these perspectives, we can set higher values. 

\begin{landscape}
	\begin{figure}
		\centering
		\includegraphics[width=1\textheight]{Images/cfr_all}
		\caption{Comparison of training accuracy curves of the four TensorFlow models with different number of features}
		\label{fig:cfr_all}
	\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
	\centering
	\includegraphics[width=1\textheight]{Images/cfr_2_4_vs_4_8}
	\caption{Comparison of training accuracy curves of two TensorFlow models with lower number of features}
	\label{fig:cfr_low}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
	\centering
	\includegraphics[width=1\textheight]{Images/cfr_8_16_vs_16_32}
	\caption{Comparison of training accuracy curves of two TensorFlow models with higher number of features}
	\label{fig:cfr_high}
\end{figure}
\end{landscape}

\subsection{Training time}

A non-trivial aspect that common people with limited computing power on their laptops always consider is: how much time does it take? It is a legitimate question, and we will promptly broach the subject.

Despite we do not gathered data about execution times, as we said before, we take some notes about. Consider data in Table \ref{tab:approx_times} non precise, but indicative.

\begin{table}
	\centering
	\begin{tabular}{cc}
		\hline 
		& \textbf{Number of features} \\ 
		\hline 
		\textbf{Model 2\_4} & 7' 30'' \\ 
		\textbf{Model 4\_8} & 8' 50'' \\ 
		\textbf{Model 8\_16} & 12' 15'' \\ 
		\textbf{Model 16\_32} & 22' 30'' \\ 
		\hline 
	\end{tabular}
	\caption{Approximate times measurements of diverse TensorFlow models executions}
	\label{tab:approx_times}
\end{table}

In order to complete time measurements in Table \ref{tab:approx_times}, here are some technical informations about relevant hardware characteristics:
\begin{itemize}
	\item Ultrabook Toshiba Satellite;
	\item Processor Intel Core i5;
	\item CPU frequency $1.7$ GHz;
	\item OS Ubuntu 16.10;
	\item No cluster;
	\item No dedicated GPU.
\end{itemize}

\subsection{Influences of parameters on the network}

What we are about to say has already been said in the previous pages, while we were analyzing all the perspectives. Now we try to summarize the aspect of how diverse numbers of features influenced network performances and times.

It is clear that better trainings can be obtained with higher number of features. But it is also clear that there in not direct proportion between number of features and training speed, stability, accuracy and time execution; so we have to conclude that increasing this key parameter is the way to improve our \acs{CNN}, but how to increase the values is at your own risk, according to:
\begin{itemize}
	\item computing power availability;
	\item time availability;
	\item desired performances.
\end{itemize}

We can not leave out an important observation: training a high-valued parameters model can be expensive, very expensive, but we have to consider that this phase is only required the first time, and then we can have a well trained network with very good performances. It might be worth it.

\section{Analysis of test accuracy}

After a long and in-depth analysis of the training phase, we can enjoy the results obtained evaluating the model in test phase. It is even more important than training accuracies because here we can analyze how powerful is the model we built and if it generalizes well.

We can evaluate generalization capabilities thanks to a part of \acs{MNIST} dataset which provide images not used during training phase. Providing images the model has never seen grant us a good checking of its capabilities.

As introduced in Chapter \ref{ch:basic_tf_model} focusing on \acs{MNIST} dataset, a large number of different classifiers have been trained on this dataset with different results. In Table \ref{tab:test_accuracies} we gather the test accuracies of our four TensorFlow models, here we observe clearly that we have a rapid reduction of the increase in test accuracy values. Indeed, on balance, from a model with 2 and 4 features to a model with 4 and 8 features we increase the final performance of $0.588 \%$, passing to 8 and 16 features we add just $0.257 \%$ more and finally $0.132 \%$ more. The average ratio between these values is about 2, it means that every time we double the number of features, we halve the improvement.

\begin{table}
	\centering
	\begin{tabular}{cc}
		\hline
		& \textbf{Test accuracies} \\
		\hline
		\textbf{Model 2\_4} & $98.145334 \%$ \\
		\textbf{Model 4\_8} & $98.733334 \%$ \\
		\textbf{Model 8\_16} & $98.990000 \%$ \\
		\textbf{Model 16\_32} & $99.122001 \%$ \\
		\hline
	\end{tabular}
	\caption{Test accuracies of the four TensorFlow models with different number of features}
	\label{tab:test_accuracies}
\end{table}